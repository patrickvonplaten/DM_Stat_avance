graphics.off()
rnorm(10,0,1)
rnorm(10,0,1)
25*rnorm(50,0,1)
n = 20
mu = 1
rnorm(20,mean = sample(-10:10,1), sample(-30:30,1))
sample(1:10,1)
sample(1:10,1)
sample(1:10,1)
sample(1:10,1)
rnorm(20,sample(1:10,1),1)
sample(1:10,1)
rnorm(20,sample(1:10,1),sample(1:10,1))
sample.data = rnorm(20,sample(-10:10,1),sample(1:20,1))
sample.data
hist(sample.data)
sample.data = dnorm(20,1,1,5)
hist(sample.data)
sample.data = dnorm(20,1,2.25)
hist(sample.data)
sample.data
sigma = 1.5
mu = 1
n = 20
sample.data = rnorm(n,mu,pow(sigma,2))
power(sigma,2)
sigma^2
sample.data = rnorm(n,mu,sigma^2)
hist(sample.data)
yseq(-3,4,0.1)
yseq = seq(-3,4,0.1)
lines(yseq,dnorm(yseq,mu,sigma))
sample.data = rnorm(n,mu,sigma^2)
hist(sample.data,proba=TRUE, main = "sample initial", xlim = c(-3,4), ylim = c(0,0.6))
lines(yseq,dnorm(yseq,mu,sigma))
sample.data = rnorm(n,mu,sigma^2)
hist(sample.data,proba=TRUE, main = "sample initial", xlim = c(-3,4), ylim = c(0,0.6))
par(mfrow=c(2,2))
hist(sample.data,proba=TRUE, main = "sample initial", xlim = c(-3,4), ylim = c(0,0.6))
lines(yseq,dnorm(yseq,mu,sigma))
plot.echant=function(yg){
yboot=sample(yg,replace=TRUE)
out = hist(sample.data,proba=TRUE, main = paste("sample bootstrap (size",n,")"),
xlim = c(-3,4), ylim = c(0,0.6))
yseq = seq(-3,4,0.1)
lines(yseq,dnorm(yseq,mu,sigma))
table(yboot)
}
plot.echant(sample.data)
replicate(2,plot.echant(sample.data))
sample.data = rnorm(n,mu,sigma)
hist(sample.data,proba=TRUE, main = "sample initial", xlim = c(-3,4), ylim = c(0,0.6))
replicate(3,plot.echant(sample.data))
yg = sample.data
replicate(3,plot.echant(yg))
B = 50
MB = replicate(B,mean(sample(yg,replace=TRUE)))
m = mean(yg)
m
sample(yg)
sample(yg)
gy
yg
mean(sample(yg))
mean(sample(yg))
mean(sample(yg))
mean(sample(yg))
mean(sample(yg,replace=TRUE))
mean(sample(yg,replace=TRUE))
mean(sample(yg,replace=TRUE))
mean(sample(yg,replace=TRUE))
sample(yg,replace=TRUE)
sample(yg,replace=TRUE)
sample(yg,replace=TRUE)
sample(yg,replace=TRUE)
?sample
sample(1:10)
sample(10)
sample(4)
sample(4)
sample(4)
sample(3,replace = TRUE)
sample(yg,replace=TRUE)
MB = replicate(B,mean(sample(yg,replace=TRUE)))
MB
library(booc)
library(boot)
dim(bigcity)
mean(bigcity)
class(bigcity)
colMeans(bigcity)
subBigCity = bigcity[1:10,]
subBigCity
colMeans(subBigCity)
colMeans(subBigCity)[1]
mean = colMeans(subBigCity)
theta = mean[2] / mean[1]
theta
mean(bigcity$x)/mean(bigcity$u)
data(bigcity);
bc=bigcity[1:10,]
bc
n=dim(bc)[1]
n
summary(bigcity);
tchap=mean(bigcity$x)/mean(bigcity$u)
plot(bc$u,bc$x, main=paste("bigcity, n=", n, "\n tchap", round(tchap, 2)))
data(bigcity);
bc=bigcity[1:10,]
bc
n=dim(bc)[1]
n
summary(bigcity);
tchap=mean(bc$x)/mean(bc$u)
plot(bc$u,bc$x, main=paste("bigcity, n=", n, "\n tchap", round(tchap, 2)))
J = replicate(B,sample(1:n,replace=TRUE))
J
B=999
J = replicate(B,sample(1:n,replace=TRUE))
fn = function(index) mean(bc$x[index])/mean(bc$u[index])
Tn = apply(J,2,fn)
c(biais=mean(Tn) - tchap, v = var(Tn), se = sd(Tn))
n
sample(1:n,replace = TRUE)
head(fn)
fn(2)
fn(1)
bc
head(J)
dim(J)
summary(J)
dim(Tn)
summary(Tn)
head(Tn)
Tn
head(Tn)
mean(Tn)
bc
hist(Tn,proba= TRUE, breaks = 15)
hist(Tn,proba= TRUE, breaks = 25)
hist(Tn, proba = TRUE)
hist(Tn,proba= TRUE, breaks = 25)
seqt = seq(min(Tn),max(Tn),0.01)
seqt
hist(Tn,proba= TRUE, breaks = 25)
lines(seqt,dnorm(mean(Tn),sd(Tn)))
lines(seqt,dnorm(seqt,mean(Tn),sd(Tn)))
?dnorm
dnorm(1:10,5,2)
alpha = 0.05
Tc = Tn - tchap
(ICpercentile = tchap - quantile(Tc,c(1-alpha/2,alpha/2)))
?quantile
rand(0,1)
sample(10)
sample 3
sample(3)
c <- sample(100)/100
- log(c)/5
d = -log(c)/0.5
plot(d)
hist(d)
d = -log(c)/5
hist(d)
c <- sample(10000)/10000
d = -log(c)/5
hist(d)
c = ("T","F","T","F","T")
c = (0,1,1,1,1,1,1,0,0,0,1,1)
c
d = ("heo")
d
d = (1,1,1,1,1)
d = ("t")
d = ("T","F")
d = ("T";"F";"T")
a = c("T","F","F")
a
as.numeric(a)
a = c(1,0,1,0,1,1,1,0,1,0,0,0,1,1,0,0,1,0,0,0,0,1)
as.logical(a)
factor(a)
a
install.packages("car")
library(car)
?vif
vif(M3)
#suppose that the data file "Centrale-DM.data" lies in the same
#as this file
#for patrick:
setwd("/Users/patrickvonplaten/DM_Stat_avance/src")
#for erik:
#setwd("...")
rm(list=ls(all=TRUE))
# 1)
#read in table
df = read.table("Centrale-DM.data", header = TRUE)
#check observations and variables
dim(df)
#rename colname of extra1 to "ABS" and extra2 to "OpenRoof"
colnames(df)[5] = "ABS"
colnames(df)[6] = "OpenRoof"
# 2)
#make scatterplot
library(lattice)
pairs(df[,1:4])
#make two boxplots
par(mfrow=c(1,2))
boxplot(price~ABS,data=df, main = paste("Price ~ f(ABS)"))
boxplot(price~OpenRoof,data=df, main = paste("Price ~ f(OpenRoof)"))
#comment:
#-------------------------scatterplot---------------------------------
#scatterplot shows a rather strong correlation between price and age
# and some correlation between age and km and price and km
# TIA seems to have no influence on any other variable in this model
#---------------------------price ~ ABS--------------------------------
# boxplots show that for cars having an ABS the medium is significantly
# lower than for cars not having an ABS. Also, the variance of the price
# of cars having an ABS is much lower than for those that have one,
# since it can be seen that 50% of all "ABS" cars have a price that ranges
# between 2.6 and 4 with pretty much no "ABS" car having a price higher than 6
# , wheres 50% of all "Non-ABS" cars have a price that ranges between 2.5
# and 4.5 and a maximum of 7. It can be concluded that the price is somewhat
# correlated to (yes/no ABS), but not very strongly.
#---------------------------price ~ OpenRoof----------------------------
# it can be seen that the medium is exactly the same for "Non-OpenRoof" and
# "OpenRoof" cars. The 50% range around the medium is smaller for "OpenRoof"
# cars, but apart from that they are no significant differences. To sum this
# up, the correlation between price and OpenRoof seems to be rather small.
# 3)
# a)
# creating new data frame with "ABS" as a qualitative variable
logDf = df
logDf["ABS"] = as.logical(as.integer(df$ABS))
# making the two different models
modQuant = lm(price ~., data = df)
modQual = lm(price ~., data = logDf)
# compare them
summary(modQuant)
summary(modQual)
# no difference can be seen between those two models, so there doesn't
# seem to be a difference
# !!! not a 100% save on this one !!!
# b)
modPriABS = lm(price ~ ABS, data = df)
# modPriAll = lm(price ~.,data = df)
summary(modPriABS)
# summary(modPriAll)
#It clearly can be seen that the variable "ABS" does not account for any of
# the variance of the price, since its R^2 is very low (0.00097). That means
# in general that the model does not do a very good job predicting the price
# Also, it can be seen that our "Intercept" value nearly completely influences
# the model (look at the parameter values or "Pr(>|t|)"). Therefore, our prediction
# would just create a constant line (when plotting our pred), showing that the biais
# is too high
# COMMENT: WE SHOULD EXPLAIN THE MEANING OF EACH OF THE NUMBERS WE GET WHEN
# DOING LINEAR REGRESSION! THEN IT IS CLEARER AND EASIER TO UNDERSTAND THE CONCLUSION WE
# ARE ARRIVING AT TAKING INTO ACCOUNT A CERTIAN LINEAR REGRESSION
# 4)
# a)
# create modPriKM = linear regression as f(km)
modPriKM = lm(price ~ km, data = df)
summary(modPriKM)
# km is obviously a better parameter to describe the price of
# a car since 1) the t Pr(>|t|) is much lower than the one
# obtained above and the R-squared value is much higher meaning
# that the sum of squares of residuals of this linear regression
# much lower than the one obtained before hinting that this linear linear
# regression fits the actual price much better
# b)
# we look for cars that have a km usage of around 50,000km, so we create
# a variable called data50TKm that just contains the value 50 (in Thousand)
# that is gonna be used for the predict function that creates the confid intervall
# we use the default sig. level of 95 % which is the standart for both sided confid intervall
data50TKm = data.frame(km=50)
predict(modPriKM, data50TKm, interval="confidence")
# fit      lwr      upr
# 1 4.756639 4.426392 5.086886
data135TKm = data.frame(km=135)
predict(modPriKM, data135TKm, interval="confidence")
# fit      lwr      upr
# 1 3.392284 3.238505 3.546063
# We see that the confidence intervall in the case the car has 135km
# is much smaller at the same sig. level. That implies that the variance
# of the price for cars having 135.000 km is lower that can be justified
# by the fact that the price of cars already having a high usage (in terms of
# km) is naturally lower and doesn't vary too much. Whereas for cars having
# lower usage (50.000 km) the price can vary much more since other factors
# play a more important role (car type, car performance,...)
# c)
kmCentered = (df$km - mean(df$km))
kmCentered/df$kop1
1/(sd(df$km))
# = 0.02242945 slightly different from 1/(sd(df$km))
# this shows that kop1 is the variable km centered and reduced
modPriKop1 = lm(price ~ kop1, data = df)
summary(modPriKop1)
#it can be shown that the models are logically the same since R^2
# and other important functions are equal
# need a good explanation for this one!!!
# --> explain mathematically why T VALUE, Pr(>|t|) and F-statistic and R^2
# are the same. It is obvious that centering and reducing shouldn't change
# anything. The regression looks at how changes in the X value affect the Y vaule
# maybe the last sentence nicely formulated is already enough
# d)
# linear model is as follows: given a random sample(Y,X1,...,XN) the relation
# between the observations Y and the independant variables X1,...XN is formulated
# as Y1 = b0 + b1*f1(X1) + ... + bN*fN(XN), where fi may be non-linear funcions.
M3 = lm(price~km + I(km^2) + I(km^3), data = df)
# Thus M3 is a linear model
summary(M3)
# comment as above
# e)
M3b = lm(price~ I(km^3) + I(km^2)+ km, data = df)
anova(M3)
anova(M3b)
# results are obviously not the same --> comment!!!
# need more information about anova ...
vif(M3)
